{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10901489,"sourceType":"datasetVersion","datasetId":6775341}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import resample\nfrom scipy.fft import fft\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom imblearn.over_sampling import SMOTE\n\n#Dataset Path\ndataset_dir = \"/kaggle/input/wesad-full-dataset/WESAD/\"  # Adjust path\nsampling_rates = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4}\nwindow_size = 30  # seconds\n\n# Load and Preprocess Data\ndef load_data():\n    all_subjects_data = []\n    stress_count = 0\n    non_stress_count = 0\n\n    for subject in os.listdir(dataset_dir):\n        subject_path = os.path.join(dataset_dir, subject, f\"{subject}.pkl\")\n        if os.path.exists(subject_path):\n            with open(subject_path, 'rb') as file:\n                data = pickle.load(file, encoding='latin1')\n\n            signal_data = data['signal']['wrist']\n            labels = data['label']\n\n            # Resample signals to align with EDA length\n            bvp_resampled = resample(signal_data['BVP'], len(signal_data['EDA']))\n            acc_resampled = resample(signal_data['ACC'], len(signal_data['EDA']), axis=0)\n\n            window_stride = sampling_rates['EDA'] * window_size  # 4 Hz * 30s = 120 samples\n            for i in range(0, len(signal_data['EDA']) - window_stride, window_stride):\n                label = labels[i]\n                if label == 1:\n                    stress_count += 1\n                else:\n                    non_stress_count += 1\n\n                all_subjects_data.append({\n                    'EDA': signal_data['EDA'][i : i + window_stride],\n                    'BVP': bvp_resampled[i : i + window_stride],\n                    'TEMP': signal_data['TEMP'][i : i + window_stride],\n                    'ACC_X': acc_resampled[i : i + window_stride, 0],\n                    'ACC_Y': acc_resampled[i : i + window_stride, 1],\n                    'ACC_Z': acc_resampled[i : i + window_stride, 2],\n                    'Label': label\n                })\n\n    print(\"Stress Samples After Windowing:\", stress_count)\n    print(\"Non-Stress Samples After Windowing:\", non_stress_count)\n    return all_subjects_data\n\n#Feature Extraction\ndef extract_features(data_list):\n    feature_dict = {col: [] for col in ['EDA', 'BVP', 'TEMP', 'ACC_X', 'ACC_Y', 'ACC_Z']}\n    labels = []\n\n    for entry in data_list:\n        labels.append(entry['Label'])\n        for col in feature_dict.keys():\n            feature_dict[col].append(entry[col])\n\n    print(\"Stress Samples After Feature Extraction:\", labels.count(1))\n    print(\"Non-Stress Samples After Feature Extraction:\", labels.count(0))\n\n    for col in feature_dict.keys():\n        feature_dict[col] = np.stack(feature_dict[col])\n\n    feature_arrays = []\n    for col in feature_dict.keys():\n        feature_arrays.append(stat_features(feature_dict[col]))\n        feature_arrays.append(freq_features(feature_dict[col], sampling_rates[col.split('_')[0]]))\n\n    feature_array = np.hstack(feature_arrays)\n    feature_df = pd.DataFrame(feature_array)\n    feature_df['Label'] = labels\n    return feature_df\n\n#Helper Functions for Feature Extraction\ndef stat_features(arr):\n    return np.column_stack([\n        np.mean(arr, axis=1), np.std(arr, axis=1), np.var(arr, axis=1),\n        np.min(arr, axis=1), np.max(arr, axis=1), np.median(arr, axis=1),\n        skew(arr, axis=1), kurtosis(arr, axis=1),\n        np.sqrt(np.mean(arr**2, axis=1))\n    ])\n\ndef freq_features(arr, sampling_rate):\n    fft_vals = np.abs(fft(arr, axis=1))\n    fft_freqs = np.fft.fftfreq(arr.shape[1], d=1/sampling_rate)\n    dominant_freq = fft_freqs[np.argmax(fft_vals, axis=1)]\n    spectral_energy = np.sum(fft_vals**2, axis=1)\n    return np.column_stack([dominant_freq, spectral_energy])\n\n#Load and Process Data\ndata_list = load_data()\ndf = extract_features(data_list)\ndf.dropna(inplace=True)\n\n# Train-Test Split\nstress_samples = df[df['Label'] == 1]\nnon_stress_samples = df[df['Label'] == 0]\n\nX_stress_train, X_stress_test, y_stress_train, y_stress_test = train_test_split(\n    stress_samples.drop(columns=['Label']), stress_samples['Label'], test_size=0.3, random_state=42\n)\n\nX_no_stress_train, X_no_stress_test, y_no_stress_train, y_no_stress_test = train_test_split(\n    non_stress_samples.drop(columns=['Label']), non_stress_samples['Label'], test_size=0.3, random_state=42\n)\n\nprint(\"Stress Samples in Training Set:\", len(X_stress_train))\nprint(\"Stress Samples in Test Set:\", len(X_stress_test))\nprint(\"Non-Stress Samples in Training Set:\", len(X_no_stress_train))\nprint(\"Non-Stress Samples in Test Set:\", len(X_no_stress_test))\n\n#Combine stress and no-stress sets to form final train and test sets\nX_train = pd.concat([X_stress_train, X_no_stress_train])\ny_train = pd.concat([y_stress_train, y_no_stress_train])\nX_test = pd.concat([X_stress_test, X_no_stress_test])\ny_test = pd.concat([y_stress_test, y_no_stress_test])\n\nprint(\"Total Samples in Training Set:\", len(X_train))\nprint(\"Stress Samples in Training Set:\", y_train.value_counts()[1])\nprint(\"Non-Stress Samples in Training Set:\", y_train.value_counts()[0])\nprint(\"Total Samples in Test Set:\", len(X_test))\nprint(\"Stress Samples in Test Set:\", y_test.value_counts()[1])\nprint(\"Non-Stress Samples in Test Set:\", y_test.value_counts()[0])\n\n#Apply SMOTE if needed\nif len(y_train.unique()) > 1:\n    smote = SMOTE(sampling_strategy=1.0, random_state=42)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n    print(\"Samples in Training Set After SMOTE:\", len(X_train))\n    print(\"Stress Samples in Training Set After SMOTE:\", y_train.value_counts()[1])\n    print(\"Non-Stress Samples in Training Set After SMOTE:\", y_train.value_counts()[0])\n\n#Train Model\nrf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\nrf.fit(X_train, y_train)\n\n#Evaluate Model\ny_pred = rf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:12:49.040670Z","iopub.execute_input":"2025-03-13T15:12:49.041181Z","iopub.status.idle":"2025-03-13T15:15:57.245723Z","shell.execute_reply.started":"2025-03-13T15:12:49.041143Z","shell.execute_reply":"2025-03-13T15:15:57.244221Z"}},"outputs":[{"name":"stdout","text":"Stress Samples After Windowing: 31\nNon-Stress Samples After Windowing: 2858\nStress Samples After Feature Extraction: 31\nNon-Stress Samples After Feature Extraction: 2858\nStress Samples in Training Set: 21\nStress Samples in Test Set: 10\nNon-Stress Samples in Training Set: 2000\nNon-Stress Samples in Test Set: 858\nTotal Samples in Training Set: 2021\nStress Samples in Training Set: 21\nNon-Stress Samples in Training Set: 2000\nTotal Samples in Test Set: 868\nStress Samples in Test Set: 10\nNon-Stress Samples in Test Set: 858\nSamples in Training Set After SMOTE: 4000\nStress Samples in Training Set After SMOTE: 2000\nNon-Stress Samples in Training Set After SMOTE: 2000\nAccuracy: 0.9988479262672811\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       858\n           1       1.00      0.90      0.95        10\n\n    accuracy                           1.00       868\n   macro avg       1.00      0.95      0.97       868\nweighted avg       1.00      1.00      1.00       868\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Reload the dataset and check label distribution without modifying the main code\n\nimport pickle\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import resample\n\n# ✅ Dataset Path (Make sure this matches your dataset location)\ndataset_dir = \"/kaggle/input/wesad-full-dataset/WESAD/\"  \nsampling_rates = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4}\nwindow_size = 30  # seconds\n\n# ✅ Load Data Function\ndef load_data():\n    all_subjects_data = []\n    for subject in os.listdir(dataset_dir):\n        subject_path = os.path.join(dataset_dir, subject, f\"{subject}.pkl\")\n        if os.path.exists(subject_path):\n            with open(subject_path, 'rb') as file:\n                data = pickle.load(file, encoding='latin1')\n            \n            labels = data['label']\n            all_subjects_data.append(pd.DataFrame({'Label': labels}))\n\n    return pd.concat(all_subjects_data, ignore_index=True)\n\n# ✅ Load and Print Label Distribution\ndf = load_data()\nprint(\"Label Distribution in Full Dataset:\\n\", df['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:24:51.672612Z","iopub.execute_input":"2025-03-13T15:24:51.673326Z","iopub.status.idle":"2025-03-13T15:27:05.088328Z","shell.execute_reply.started":"2025-03-13T15:24:51.673281Z","shell.execute_reply":"2025-03-13T15:27:05.086895Z"}},"outputs":[{"name":"stdout","text":"Label Distribution in Full Dataset:\n Label\n0    27654897\n1    12327702\n4     8264199\n2     6976201\n3     3902501\n7      576802\n6      552998\n5      552300\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2}]}