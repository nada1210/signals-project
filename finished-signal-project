{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629a1e7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:02:43.100727Z",
     "iopub.status.busy": "2025-05-09T13:02:43.100293Z",
     "iopub.status.idle": "2025-05-09T13:03:26.747254Z",
     "shell.execute_reply": "2025-05-09T13:03:26.746004Z"
    },
    "papermill": {
     "duration": 43.65255,
     "end_time": "2025-05-09T13:03:26.749182",
     "exception": false,
     "start_time": "2025-05-09T13:02:43.096632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.3.2\r\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting imbalanced-learn==0.11.0\r\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Collecting numpy<2.0,>=1.17.3 (from scikit-learn==1.3.2)\r\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn==1.3.2)\r\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn==1.3.2)\r\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2)\r\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\r\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, imbalanced-learn\r\n",
      "  Attempting uninstall: threadpoolctl\r\n",
      "    Found existing installation: threadpoolctl 3.6.0\r\n",
      "    Uninstalling threadpoolctl-3.6.0:\r\n",
      "      Successfully uninstalled threadpoolctl-3.6.0\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "  Attempting uninstall: joblib\r\n",
      "    Found existing installation: joblib 1.4.2\r\n",
      "    Uninstalling joblib-1.4.2:\r\n",
      "      Successfully uninstalled joblib-1.4.2\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.15.2\r\n",
      "    Uninstalling scipy-1.15.2:\r\n",
      "      Successfully uninstalled scipy-1.15.2\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: imbalanced-learn\r\n",
      "    Found existing installation: imbalanced-learn 0.13.0\r\n",
      "    Uninstalling imbalanced-learn-0.13.0:\r\n",
      "      Successfully uninstalled imbalanced-learn-0.13.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\r\n",
      "nilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "ibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "ibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.11.0 joblib-1.5.0 numpy-1.26.4 scikit-learn-1.3.2 scipy-1.15.3 threadpoolctl-3.6.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --force-reinstall scikit-learn==1.3.2 imbalanced-learn==0.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23137cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:03:26.760399Z",
     "iopub.status.busy": "2025-05-09T13:03:26.759981Z",
     "iopub.status.idle": "2025-05-09T13:06:27.456782Z",
     "shell.execute_reply": "2025-05-09T13:06:27.455428Z"
    },
    "papermill": {
     "duration": 180.704562,
     "end_time": "2025-05-09T13:06:27.458523",
     "exception": false,
     "start_time": "2025-05-09T13:03:26.753961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Samples: 587\n",
      "Non-Stress Samples: 2302\n",
      "Stress Samples After Feature Extraction: 587\n",
      "Non-Stress Samples After Feature Extraction: 1314\n",
      "Stress Samples in Training Set: 410\n",
      "Stress Samples in Test Set: 177\n",
      "Non-Stress Samples in Training Set: 919\n",
      "Non-Stress Samples in Test Set: 395\n",
      "Total Samples in Training Set: 1329\n",
      "Stress Samples in Training Set: 410\n",
      "Non-Stress Samples in Training Set: 919\n",
      "Total Samples in Test Set: 572\n",
      "Stress Samples in Test Set: 177\n",
      "Non-Stress Samples in Test Set: 395\n",
      "Samples in Training Set After SMOTE: 1838\n",
      "Stress Samples in Training Set After SMOTE: 919\n",
      "Non-Stress Samples in Training Set After SMOTE: 919\n",
      "Accuracy: 0.8671328671328671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       395\n",
      "           1       0.75      0.85      0.80       177\n",
      "\n",
      "    accuracy                           0.87       572\n",
      "   macro avg       0.84      0.86      0.85       572\n",
      "weighted avg       0.87      0.87      0.87       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import resample\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Dataset Path\n",
    "dataset_dir = \"/kaggle/input/wesad-full-dataset/WESAD\"  # Adjust path\n",
    "sampling_rates = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4}\n",
    "window_size = 30  # seconds\n",
    "\n",
    "# Load and Preprocess Data\n",
    "def load_data():\n",
    "    all_subjects_data = []\n",
    "    stress_count = 0  # Initialize here (outside all loops)\n",
    "    non_stress_count = 0  # Initialize here (outside all loops)\n",
    "\n",
    "    for subject in os.listdir(dataset_dir):\n",
    "        subject_path = os.path.join(dataset_dir, subject, f\"{subject}.pkl\")\n",
    "        if os.path.exists(subject_path):\n",
    "            with open(subject_path, 'rb') as file:\n",
    "                data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "            signal_data = data['signal']['wrist']\n",
    "            labels = data['label']\n",
    "\n",
    "            # Resample signals\n",
    "            bvp_resampled = resample(signal_data['BVP'], len(signal_data['EDA']))\n",
    "            acc_resampled = resample(signal_data['ACC'], len(signal_data['EDA']), axis=0)\n",
    "\n",
    "            window_stride = sampling_rates['EDA'] * window_size\n",
    "            for i in range(0, len(signal_data['EDA']) - window_stride, window_stride):\n",
    "                label_index = min(int(i * (700 / 4)), len(labels) - 1)\n",
    "                label = labels[label_index]\n",
    "                \n",
    "                if label == 1:\n",
    "                    stress_count += 1\n",
    "                else:\n",
    "                    non_stress_count += 1  # Now properly defined\n",
    "\n",
    "                all_subjects_data.append({\n",
    "                    'EDA': signal_data['EDA'][i:i+window_stride],\n",
    "                    'BVP': bvp_resampled[i:i+window_stride],\n",
    "                    'TEMP': signal_data['TEMP'][i:i+window_stride],\n",
    "                    'ACC_X': acc_resampled[i:i+window_stride, 0],\n",
    "                    'ACC_Y': acc_resampled[i:i+window_stride, 1],\n",
    "                    'ACC_Z': acc_resampled[i:i+window_stride, 2],\n",
    "                    'Label': label\n",
    "                })\n",
    "\n",
    "    print(\"Stress Samples:\", stress_count)  # Now accessible\n",
    "    print(\"Non-Stress Samples:\", non_stress_count)  # Now accessible\n",
    "    return all_subjects_data\n",
    "    \n",
    "#Feature Extraction\n",
    "def extract_features(data_list):\n",
    "    feature_dict = {col: [] for col in ['EDA', 'BVP', 'TEMP', 'ACC_X', 'ACC_Y', 'ACC_Z']}\n",
    "    labels = []\n",
    "\n",
    "    for entry in data_list:\n",
    "        labels.append(entry['Label'])\n",
    "        for col in feature_dict.keys():\n",
    "            feature_dict[col].append(entry[col])\n",
    "\n",
    "    print(\"Stress Samples After Feature Extraction:\", labels.count(1))\n",
    "    print(\"Non-Stress Samples After Feature Extraction:\", labels.count(0))\n",
    "\n",
    "    for col in feature_dict.keys():\n",
    "        feature_dict[col] = np.stack(feature_dict[col])\n",
    "\n",
    "    feature_arrays = []\n",
    "    for col in feature_dict.keys():\n",
    "        feature_arrays.append(stat_features(feature_dict[col]))\n",
    "        feature_arrays.append(freq_features(feature_dict[col], 4))  # All resampled to 4 Hz\n",
    "\n",
    "    feature_array = np.hstack(feature_arrays)\n",
    "    feature_df = pd.DataFrame(feature_array)\n",
    "    feature_df['Label'] = labels\n",
    "    return feature_df\n",
    "\n",
    "#Helper Functions for Feature Extraction\n",
    "def stat_features(arr):\n",
    "    return np.column_stack([\n",
    "        np.mean(arr, axis=1), np.std(arr, axis=1), np.var(arr, axis=1),\n",
    "        np.min(arr, axis=1), np.max(arr, axis=1), np.median(arr, axis=1),\n",
    "        skew(arr, axis=1), kurtosis(arr, axis=1),\n",
    "        np.sqrt(np.mean(arr**2, axis=1))\n",
    "    ])\n",
    "\n",
    "def freq_features(arr, sampling_rate):\n",
    "    fft_vals = np.abs(fft(arr, axis=1))\n",
    "    fft_freqs = np.fft.fftfreq(arr.shape[1], d=1/sampling_rate)\n",
    "    dominant_freq = fft_freqs[np.argmax(fft_vals, axis=1)]\n",
    "    spectral_energy = np.sum(fft_vals**2, axis=1)\n",
    "    return np.column_stack([dominant_freq, spectral_energy])\n",
    "\n",
    "#Load and Process Data\n",
    "data_list = load_data()\n",
    "df = extract_features(data_list)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Train-Test Split\n",
    "stress_samples = df[df['Label'] == 1]\n",
    "non_stress_samples = df[df['Label'] == 0]\n",
    "\n",
    "X_stress_train, X_stress_test, y_stress_train, y_stress_test = train_test_split(\n",
    "    stress_samples.drop(columns=['Label']), stress_samples['Label'], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_no_stress_train, X_no_stress_test, y_no_stress_train, y_no_stress_test = train_test_split(\n",
    "    non_stress_samples.drop(columns=['Label']), non_stress_samples['Label'], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Stress Samples in Training Set:\", len(X_stress_train))\n",
    "print(\"Stress Samples in Test Set:\", len(X_stress_test))\n",
    "print(\"Non-Stress Samples in Training Set:\", len(X_no_stress_train))\n",
    "print(\"Non-Stress Samples in Test Set:\", len(X_no_stress_test))\n",
    "\n",
    "#Combine stress and no-stress sets to form final train and test sets\n",
    "X_train = pd.concat([X_stress_train, X_no_stress_train])\n",
    "y_train = pd.concat([y_stress_train, y_no_stress_train])\n",
    "X_test = pd.concat([X_stress_test, X_no_stress_test])\n",
    "y_test = pd.concat([y_stress_test, y_no_stress_test])\n",
    "\n",
    "print(\"Total Samples in Training Set:\", len(X_train))\n",
    "print(\"Stress Samples in Training Set:\", y_train.value_counts()[1])\n",
    "print(\"Non-Stress Samples in Training Set:\", y_train.value_counts()[0])\n",
    "print(\"Total Samples in Test Set:\", len(X_test))\n",
    "print(\"Stress Samples in Test Set:\", y_test.value_counts()[1])\n",
    "print(\"Non-Stress Samples in Test Set:\", y_test.value_counts()[0])\n",
    "\n",
    "#Apply SMOTE if needed\n",
    "if len(y_train.unique()) > 1:\n",
    "    smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Samples in Training Set After SMOTE:\", len(X_train))\n",
    "    print(\"Stress Samples in Training Set After SMOTE:\", y_train.value_counts()[1])\n",
    "    print(\"Non-Stress Samples in Training Set After SMOTE:\", y_train.value_counts()[0])\n",
    "\n",
    "#Train Model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate Model\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8d31b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:06:27.469429Z",
     "iopub.status.busy": "2025-05-09T13:06:27.468958Z",
     "iopub.status.idle": "2025-05-09T13:07:39.095155Z",
     "shell.execute_reply": "2025-05-09T13:07:39.094174Z"
    },
    "papermill": {
     "duration": 71.638271,
     "end_time": "2025-05-09T13:07:39.101560",
     "exception": false,
     "start_time": "2025-05-09T13:06:27.463289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution in Full Dataset:\n",
      " Label\n",
      "0    27654897\n",
      "1    12327702\n",
      "4     8264199\n",
      "2     6976201\n",
      "3     3902501\n",
      "7      576802\n",
      "6      552998\n",
      "5      552300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reload the dataset and check label distribution without modifying the main code\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import resample\n",
    "\n",
    "# ✅ Dataset Path (Make sure this matches your dataset location)\n",
    "dataset_dir = \"/kaggle/input/wesad-full-dataset/WESAD/\"  \n",
    "sampling_rates = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4}\n",
    "window_size = 30  # seconds\n",
    "\n",
    "# ✅ Load Data Function\n",
    "def load_data():\n",
    "    all_subjects_data = []\n",
    "    for subject in os.listdir(dataset_dir):\n",
    "        subject_path = os.path.join(dataset_dir, subject, f\"{subject}.pkl\")\n",
    "        if os.path.exists(subject_path):\n",
    "            with open(subject_path, 'rb') as file:\n",
    "                data = pickle.load(file, encoding='latin1')\n",
    "            \n",
    "            labels = data['label']\n",
    "            all_subjects_data.append(pd.DataFrame({'Label': labels}))\n",
    "\n",
    "    return pd.concat(all_subjects_data, ignore_index=True)\n",
    "\n",
    "# ✅ Load and Print Label Distribution\n",
    "df = load_data()\n",
    "print(\"Label Distribution in Full Dataset:\\n\", df['Label'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6775341,
     "sourceId": 11631688,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 301.986102,
   "end_time": "2025-05-09T13:07:39.929230",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T13:02:37.943128",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
